---
title: Llm
description: API reference for openhands.sdk.llm
---

# Llm

**Source:** [`openhands/sdk/llm/`](https://github.com/OpenHands/software-agent-sdk/tree/main/openhands-sdk/openhands/sdk/llm/)

---

## Classes

### `LLMError`

Base class for all LLM-related exceptions.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `LLMMalformedActionError`

Exception raised when the LLM response is malformed or does not conform to the expected format.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `LLMNoActionError`

Exception raised when the LLM response does not include an action.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `LLMResponseError`

Exception raised when the LLM response does not include an action or the action is not of the expected type.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `LLMNoResponseError`

Exception raised when the LLM does not return a response, typically seen in
Gemini models.

This exception should be retried
Typically, after retry with a non-zero temperature, the LLM will return a response

#### Methods

##### `__init__(self, message: str) -> None`

---

### `LLMContextWindowExceedError`

#### Methods

##### `__init__(self, message: str) -> None`

---

### `FunctionCallConversionError`

Exception raised when FunctionCallingConverter failed to convert a non-function
call message to a function call message.

This typically happens when there's a malformed message (e.g., missing
<function=...> tags). But not due to LLM output.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `FunctionCallValidationError`

Exception raised when FunctionCallingConverter failed to validate a function
call message.

This typically happens when the LLM outputs unrecognized function call /
parameter names / values.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `FunctionCallNotExistsError`

Exception raised when an LLM call a tool that is not registered.

#### Methods

##### `__init__(self, message: str) -> None`

---

### `UserCancelledError`

#### Methods

##### `__init__(self, message: str) -> None`

---

### `OperationCancelled`

Exception raised when an operation is cancelled (e.g. by a keyboard interrupt).

#### Methods

##### `__init__(self, message: str) -> None`

---

### `LLM`

Refactored LLM: simple `completion()`, centralized Telemetry, tiny helpers.

#### Methods

##### `service_id(self) -> str`

##### `service_id(self, value: str) -> None`

##### `metrics(self) -> Metrics`

##### `restore_metrics(self, metrics: Metrics) -> None`

##### `completion(self, messages: list[Message], tools: Sequence[ToolBase] | None, _return_metrics: bool, add_security_risk_prediction: bool) -> LLMResponse`

Single entry point for LLM completion.

Normalize → (maybe) mock tools → transport → postprocess.

##### `responses(self, messages: list[Message], tools: Sequence[ToolBase] | None, include: list[str] | None, store: bool | None, _return_metrics: bool, add_security_risk_prediction: bool) -> LLMResponse`

Alternative invocation path using OpenAI Responses API via LiteLLM.

Maps Message[] -> (instructions, input[]) and returns LLMResponse.
Non-stream only for v1.

##### `vision_is_active(self) -> bool`

##### `is_caching_prompt_active(self) -> bool`

Check if prompt caching is supported and enabled for current model.


**Returns:**

boolean: True if prompt caching is supported and enabled for the given
model.

##### `uses_responses_api(self) -> bool`

Whether this model uses the OpenAI Responses API path.

##### `model_info(self) -> dict | None`

Returns the model info dictionary.

##### `format_messages_for_llm(self, messages: list[Message]) -> list[dict]`

Formats Message objects for LLM consumption.

##### `format_messages_for_responses(self, messages: list[Message]) -> tuple[str | None, list[dict[str, Any]]]`

Prepare (instructions, input[]) for the OpenAI Responses API.

- Skips prompt caching flags and string serializer concerns
- Uses Message.to_responses_value to get either instructions (system)
or input items (others)
- Concatenates system instructions into a single instructions string

##### `get_token_count(self, messages: list[Message]) -> int`

##### `load_from_json(cls, json_path: str) -> LLM`

##### `load_from_env(cls, prefix: str) -> LLM`

##### `resolve_diff_from_deserialized(self, persisted: LLM) -> LLM`

Resolve differences between a deserialized LLM and the current instance.

This is due to fields like api_key being serialized to "****" in dumps,
and we want to ensure that when loading from a file, we still use the
runtime-provided api_key in the self instance.

Return a new LLM instance equivalent to `persisted` but with
explicitly whitelisted fields (e.g. api_key) taken from `self`.

##### `is_context_window_exceeded_exception(exception: Exception) -> bool`

Check if the exception indicates a context window exceeded error.

Context window exceeded errors vary by provider, and LiteLLM does not do a
consistent job of identifying and wrapping them.

---

### `RegistryEvent`

---

### `LLMRegistry`

A minimal LLM registry for managing LLM instances by usage ID.

This registry provides a simple way to manage multiple LLM instances,
avoiding the need to recreate LLMs with the same configuration.

#### Methods

##### `__init__(self, retry_listener: Callable[[int, int], None] | None)`

Initialize the LLM registry.


**Parameters:**

retry_listener: Optional callback for retry events.

##### `subscribe(self, callback: Callable[[RegistryEvent], None]) -> None`

Subscribe to registry events.


**Parameters:**

callback: Function to call when LLMs are created or updated.

##### `notify(self, event: RegistryEvent) -> None`

Notify subscribers of registry events.


**Parameters:**

event: The registry event to notify about.

##### `usage_to_llm(self) -> dict[str, LLM]`

Access the internal usage-ID-to-LLM mapping.

##### `service_to_llm(self) -> dict[str, LLM]`

##### `add(self, llm: LLM) -> None`

Add an LLM instance to the registry.


**Parameters:**

llm: The LLM instance to register.


**Raises:**

ValueError: If llm.usage_id already exists in the registry.

##### `get(self, usage_id: str) -> LLM`

Get an LLM instance from the registry.


**Parameters:**

usage_id: Unique identifier for the LLM usage slot.


**Returns:**

The LLM instance.


**Raises:**

KeyError: If usage_id is not found in the registry.

##### `list_usage_ids(self) -> list[str]`

List all registered usage IDs.

##### `list_services(self) -> list[str]`

Deprecated alias for :meth:`list_usage_ids`.

---

### `LLMResponse`

Result of an LLM completion request.

This type provides a clean interface for LLM completion results, exposing
only OpenHands-native types to consumers while preserving access to the
raw LiteLLM response for internal use.

Attributes:
message: The completion message converted to OpenHands Message type
metrics: Snapshot of metrics from the completion request
raw_response: The original LiteLLM response (ModelResponse or
ResponsesAPIResponse) for internal use

#### Methods

##### `id(self) -> str`

Get the response ID from the underlying LLM response.

This property provides a clean interface to access the response ID,
supporting both completion mode (ModelResponse) and response API modes
(ResponsesAPIResponse).


**Returns:**

The response ID from the LLM response

---

### `MessageToolCall`

Transport-agnostic tool call representation.

One canonical id is used for linking across actions/observations and
for Responses function_call_output call_id.

#### Methods

##### `from_chat_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -> 'MessageToolCall'`

Create a MessageToolCall from a Chat Completions tool call.

##### `from_responses_function_call(cls, item: ResponseFunctionToolCall | OutputFunctionToolCall) -> 'MessageToolCall'`

Create a MessageToolCall from a typed OpenAI Responses function_call item.

Note: OpenAI Responses function_call.arguments is already a JSON string.

##### `to_chat_dict(self) -> dict[str, Any]`

Serialize to OpenAI Chat Completions tool_calls format.

##### `to_responses_dict(self) -> dict[str, Any]`

Serialize to OpenAI Responses 'function_call' input item format.

---

### `ThinkingBlock`

Anthropic thinking block for extended thinking feature.

This represents the raw thinking blocks returned by Anthropic models
when extended thinking is enabled. These blocks must be preserved
and passed back to the API for tool use scenarios.

---

### `RedactedThinkingBlock`

Redacted thinking block for previous responses without extended thinking.

This is used as a placeholder for assistant messages that were generated
before extended thinking was enabled.

---

### `ReasoningItemModel`

OpenAI Responses reasoning item (non-stream, subset we consume).

Do not log or render encrypted_content.

---

### `BaseContent`

#### Methods

##### `to_llm_dict(self) -> list[dict[str, str | dict[str, str]]]`

Convert to LLM API format. Always returns a list of dictionaries.

Subclasses should implement this method to return a list of dictionaries,
even if they only have a single item.

---

### `TextContent`

#### Methods

##### `to_llm_dict(self) -> list[dict[str, str | dict[str, str]]]`

Convert to LLM API format.

---

### `ImageContent`

#### Methods

##### `to_llm_dict(self) -> list[dict[str, str | dict[str, str]]]`

Convert to LLM API format.

---

### `Message`

#### Methods

##### `contains_image(self) -> bool`

##### `to_chat_dict(self) -> dict[str, Any]`

Serialize message for OpenAI Chat Completions.

Chooses the appropriate content serializer and then injects threading keys:
- Assistant tool call turn: role == "assistant" and self.tool_calls
- Tool result turn: role == "tool" and self.tool_call_id (with name)

##### `to_responses_value(self) -> str | list[dict[str, Any]]`

Return serialized form.

Either an instructions string (for system) or input items (for other roles).

##### `to_responses_dict(self) -> list[dict[str, Any]]`

Serialize message for OpenAI Responses (input parameter).

Produces a list of "input" items for the Responses API:
- system: returns [], system content is expected in 'instructions'
- user: one 'message' item with content parts -> input_text / input_image
(when vision enabled)
- assistant: emits prior assistant content as input_text,
and function_call items for tool_calls
- tool: emits function_call_output items (one per TextContent)
with matching call_id

##### `from_llm_chat_message(cls, message: LiteLLMMessage) -> 'Message'`

Convert a LiteLLMMessage (Chat Completions) to our Message class.

Provider-agnostic mapping for reasoning:
- Prefer `message.reasoning_content` if present (LiteLLM normalized field)
- Extract `thinking_blocks` from content array (Anthropic-specific)

##### `from_llm_responses_output(cls, output: Any) -> 'Message'`

Convert OpenAI Responses API output items into a single assistant Message.

Policy (non-stream):
- Collect assistant text by concatenating output_text parts from message items
- Normalize function_call items to MessageToolCall list

---

## Functions

### `content_to_str(contents: Sequence[TextContent | ImageContent]) -> list[str]`

Convert a list of TextContent and ImageContent to a list of strings.

This is primarily used for display purposes.

---

