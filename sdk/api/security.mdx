---
title: Security
description: API reference for openhands.sdk.security
---

# Security

**Source:** [`openhands/sdk/security/`](https://github.com/OpenHands/software-agent-sdk/tree/main/openhands-sdk/openhands/sdk/security/)

---

## Classes

### `SecurityAnalyzerBase`

Abstract base class for security analyzers.

Security analyzers evaluate the risk of actions before they are executed
and can influence the conversation flow based on security policies.

This is adapted from OpenHands SecurityAnalyzer but designed to work
with the agent-sdk's conversation-based architecture.

#### Methods

##### `security_risk(self, action: ActionEvent) -> SecurityRisk`

Evaluate the security risk of an ActionEvent.

This is the core method that analyzes an ActionEvent and returns its risk level.
Implementations should examine the action's content, context, and potential
impact to determine the appropriate risk level.


**Parameters:**

action: The ActionEvent to analyze for security risks


**Returns:**

ActionSecurityRisk enum indicating the risk level

##### `analyze_event(self, event: Event) -> SecurityRisk | None`

Analyze an event for security risks.

This is a convenience method that checks if the event is an action
and calls security_risk() if it is. Non-action events return None.


**Parameters:**

event: The event to analyze


**Returns:**

ActionSecurityRisk if event is an action, None otherwise

##### `should_require_confirmation(self, risk: SecurityRisk, confirmation_mode: bool) -> bool`

Determine if an action should require user confirmation.

This implements the default confirmation logic based on risk level
and confirmation mode settings.


**Parameters:**

risk: The security risk level of the action
confirmation_mode: Whether confirmation mode is enabled


**Returns:**

True if confirmation is required, False otherwise

##### `analyze_pending_actions(self, pending_actions: list[ActionEvent]) -> list[tuple[ActionEvent, SecurityRisk]]`

Analyze all pending actions in a conversation.

This method gets all unmatched actions from the conversation state
and analyzes each one for security risks.


**Parameters:**

conversation: The conversation to analyze


**Returns:**

List of tuples containing (action, risk_level) for each pending action

---

### `ConfirmationPolicyBase`

#### Methods

##### `should_confirm(self, risk: SecurityRisk) -> bool`

Determine if an action with the given risk level requires confirmation.

This method defines the core logic for determining whether user confirmation
is required before executing an action based on its security risk level.


**Parameters:**

risk: The security risk level of the action to be evaluated.
Defaults to SecurityRisk.UNKNOWN if not specified.


**Returns:**

True if the action requires user confirmation before execution,
False if the action can proceed without confirmation.

---

### `AlwaysConfirm`

#### Methods

##### `should_confirm(self, risk: SecurityRisk) -> bool`

---

### `NeverConfirm`

#### Methods

##### `should_confirm(self, risk: SecurityRisk) -> bool`

---

### `ConfirmRisky`

#### Methods

##### `validate_threshold(cls, v: SecurityRisk) -> SecurityRisk`

##### `should_confirm(self, risk: SecurityRisk) -> bool`

---

### `LLMSecurityAnalyzer`

LLM-based security analyzer.

This analyzer respects the security_risk attribute that can be set by the LLM
when generating actions, similar to OpenHands' LLMRiskAnalyzer.

It provides a lightweight security analysis approach that leverages the LLM's
understanding of action context and potential risks.

#### Methods

##### `security_risk(self, action: ActionEvent) -> SecurityRisk`

Evaluate security risk based on LLM-provided assessment.

This method checks if the action has a security_risk attribute set by the LLM
and returns it. The LLM may not always provide this attribute but it defaults to
UNKNOWN if not explicitly set.

---

### `SecurityRisk`

Security risk levels for actions.

Based on OpenHands security risk levels but adapted for agent-sdk.
Integer values allow for easy comparison and ordering.

#### Methods

##### `description(self) -> str`

Get a human-readable description of the risk level.

##### `get_color(self) -> str`

Get the color for displaying this risk level in Rich text.

##### `visualize(self) -> Text`

Return Rich Text representation of this risk level.

##### `is_riskier(self, other: SecurityRisk, reflexive: bool) -> bool`

Check if this risk level is riskier than another.

Risk levels follow the natural ordering: LOW is less risky than MEDIUM, which is
less risky than HIGH. UNKNOWN is not comparable to any other level.

To make this act like a standard well-ordered domain, we reflexively consider
risk levels to be riskier than themselves. That is:

for risk_level in list(SecurityRisk):
assert risk_level.is_riskier(risk_level)

# More concretely:
assert SecurityRisk.HIGH.is_riskier(SecurityRisk.HIGH)
assert SecurityRisk.MEDIUM.is_riskier(SecurityRisk.MEDIUM)
assert SecurityRisk.LOW.is_riskier(SecurityRisk.LOW)

This can be disabled by setting the `reflexive` parameter to False.


**Parameters:**

other (SecurityRisk): The other risk level to compare against.
reflexive (bool): Whether the relationship is reflexive.


**Raises:**

ValueError: If either risk level is UNKNOWN.

---

