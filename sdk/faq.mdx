---
title: FAQ
description: Frequently asked questions about the OpenHands SDK
icon: question
---

## How do I use AWS Bedrock with the SDK?

**Yes, the OpenHands SDK supports AWS Bedrock through LiteLLM.**

Since LiteLLM requires `boto3` for Bedrock requests, you need to install it alongside the SDK.

<Accordion title="Setup Instructions" icon="gear">

### Step 1: Install boto3

Install the SDK with boto3:

```bash
# Using pip
pip install openhands-sdk boto3

# Using uv
uv pip install openhands-sdk boto3

# Or when installing as a CLI tool
uv tool install openhands --with boto3
```

### Step 2: Configure Authentication

You have two authentication options:

**Option A: API Key Authentication (Recommended)**

Use the `AWS_BEARER_TOKEN_BEDROCK` environment variable:

```bash
export AWS_BEARER_TOKEN_BEDROCK="your-bedrock-api-key"
```

**Option B: AWS Credentials**

Use traditional AWS credentials:

```bash
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_REGION_NAME="us-west-2"
```

### Step 3: Configure the Model

Use the `bedrock/` prefix for your model name:

```python
from openhands.sdk import LLM, Agent

llm = LLM(
    model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0",
    # api_key is read from AWS_BEARER_TOKEN_BEDROCK automatically
)
```

For cross-region inference profiles, include the region prefix:

```python
llm = LLM(
    model="bedrock/us.anthropic.claude-3-5-sonnet-20240620-v1:0",  # US region
    # or
    model="bedrock/apac.anthropic.claude-sonnet-4-20250514-v1:0",  # APAC region
)
```

</Accordion>

For more details on Bedrock configuration options, see the [LiteLLM Bedrock documentation](https://docs.litellm.ai/docs/providers/bedrock).

## Does the agent SDK support parallel tool calling?

**Yes, the OpenHands SDK supports parallel tool calling by default.**

The SDK automatically handles parallel tool calls when the underlying LLM (like Claude or GPT-4) returns multiple tool calls in a single response. This allows agents to execute multiple independent actions before the next LLM call.

<Accordion title="How it works" icon="gear">
When the LLM generates multiple tool calls in parallel, the SDK groups them using a shared `llm_response_id`:

```python
ActionEvent(llm_response_id="abc123", thought="Let me check...", tool_call=tool1)
ActionEvent(llm_response_id="abc123", thought=[], tool_call=tool2)
# Combined into: Message(role="assistant", content="Let me check...", tool_calls=[tool1, tool2])
```

Multiple `ActionEvent`s with the same `llm_response_id` are grouped together and combined into a single LLM message with multiple `tool_calls`. Only the first event's thought/reasoning is included. The parallel tool calling implementation can be found in the [Events Architecture](/sdk/arch/events#event-types) for detailed explanation of how parallel function calling works, the [`prepare_llm_messages` in utils.py](https://github.com/OpenHands/software-agent-sdk/blob/main/openhands-sdk/openhands/sdk/agent/utils.py) which groups ActionEvents by `llm_response_id` when converting events to LLM messages, the [agent step method](https://github.com/OpenHands/software-agent-sdk/blob/main/openhands-sdk/openhands/sdk/agent/agent.py#L200-L300) where actions are created with shared `llm_response_id`, and the [`ActionEvent` class](https://github.com/OpenHands/software-agent-sdk/blob/main/openhands-sdk/openhands/sdk/event/llm_convertible/action.py) which includes the `llm_response_id` field. For more details, see the **[Events Architecture](/sdk/arch/events)** for a deep dive into the event system and parallel function calling, the **[Tool System](/sdk/arch/tool-system)** for understanding how tools work with the agent, and the **[Agent Architecture](/sdk/arch/agent)** for how agents process and execute actions.
</Accordion>

## Does the agent SDK support image content?

**Yes, the OpenHands SDK fully supports image content for vision-capable LLMs.**

The SDK supports both HTTP/HTTPS URLs and base64-encoded images through the `ImageContent` class.

<Accordion title="How to use images" icon="image">

### Check Vision Support

Before sending images, verify your LLM supports vision:

```python
from openhands.sdk import LLM
from pydantic import SecretStr

llm = LLM(
    model="anthropic/claude-sonnet-4-5-20250929",
    api_key=SecretStr("your-api-key"),
    usage_id="my-agent"
)

# Check if vision is active
assert llm.vision_is_active(), "Model does not support vision"
```

### Using HTTP URLs

```python
from openhands.sdk import ImageContent, Message, TextContent

message = Message(
    role="user",
    content=[
        TextContent(text="What do you see in this image?"),
        ImageContent(image_urls=["https://example.com/image.png"]),
    ],
)
```

### Using Base64 Images

Base64 images are supported using data URLs:

```python
import base64
from openhands.sdk import ImageContent, Message, TextContent

# Read and encode an image file
with open("my_image.png", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode("utf-8")

# Create message with base64 image
message = Message(
    role="user",
    content=[
        TextContent(text="Describe this image"),
        ImageContent(image_urls=[f"data:image/png;base64,{image_base64}"]),
    ],
)
```

### Supported Image Formats

The data URL format is: `data:<mime_type>;base64,<base64_encoded_data>`

Supported MIME types:
- `image/png`
- `image/jpeg`
- `image/gif`
- `image/webp`
- `image/bmp`

### Built-in Image Support

Several SDK tools automatically handle images:

- **FileEditorTool**: When viewing image files (`.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`, `.bmp`), they're automatically converted to base64 and sent to the LLM
- **BrowserUseTool**: Screenshots are captured and sent as base64 images
- **MCP Tools**: Image content from MCP tool results is automatically converted to base64 data URLs

### Disabling Vision

To disable vision for cost reduction (even on vision-capable models):

```python
llm = LLM(
    model="anthropic/claude-sonnet-4-5-20250929",
    api_key=SecretStr("your-api-key"),
    usage_id="my-agent",
    disable_vision=True,  # Images will be filtered out
)
```

</Accordion>

For a complete example, see the [image input example](https://github.com/OpenHands/software-agent-sdk/blob/main/examples/01_standalone_sdk/17_image_input.py) in the SDK repository.

## How do I handle MessageEvent in evaluation pipelines?

**The SDK provides utilities to automatically respond to agent messages during evaluations.**

When running evaluations, some models may send a `MessageEvent` (proposing an action or asking for confirmation) instead of directly using tools. This causes `conversation.run()` to return, even though the agent hasn't finished the task. The OpenHands benchmarks repository includes a utility function to handle this scenario.

<Accordion title="Understanding the Problem" icon="circle-question">

When an agent sends a message to the user (via `MessageEvent`) instead of using the `finish` tool, the conversation ends because it's waiting for user input. In evaluation pipelines, there's no human to respond, so the task appears incomplete even though the agent was moving in the right direction.

**Key event types:**
- `ActionEvent`: Agent uses a tool (terminal, file editor, etc.)
- `MessageEvent`: Agent sends a text message (waiting for user response)
- `FinishAction`: Agent explicitly signals task completion

The solution is to automatically send a "fake user response" when the agent sends a message, prompting it to continue working.

</Accordion>

<Accordion title="Solution: Using run_conversation_with_fake_user_response" icon="code">

The [`run_conversation_with_fake_user_response`](https://github.com/OpenHands/benchmarks/blob/main/benchmarks/utils/fake_user_response.py) function wraps your conversation execution and automatically handles agent messages:

```python
from openhands.sdk import Agent, Conversation, LLM
from openhands.sdk.conversation.state import ConversationExecutionStatus
from openhands.sdk.event import ActionEvent, Event, MessageEvent
from openhands.sdk.tool.builtins.finish import FinishAction


def fake_user_response(conversation) -> str:
    """Generate a fake user response to keep the agent working."""
    msg = (
        "Please continue working on the task on whatever approach you think is suitable.\n"
        "When you think you have solved the question, please use the finish tool and "
        "include your final answer in the message parameter of the finish tool.\n"
        "IMPORTANT: YOU SHOULD NEVER ASK FOR HUMAN HELP.\n"
    )

    # After multiple attempts, let the agent know it can give up
    events = list(conversation.state.events)
    user_msgs = [
        e for e in events
        if isinstance(e, MessageEvent) and e.source == "user"
    ]
    if len(user_msgs) >= 2:
        msg += 'If you want to give up, use the "finish" tool to finish the interaction.\n'

    return msg


def _agent_finished_with_finish_action(events: list[Event]) -> bool:
    """Check if the agent finished by calling the finish tool."""
    for event in reversed(events):
        if isinstance(event, ActionEvent):
            if event.action is not None and isinstance(event.action, FinishAction):
                return True
            return False
    return False


def _agent_sent_message(events: list[Event]) -> bool:
    """Check if the agent's last event was a message (not a tool call)."""
    for event in reversed(events):
        if isinstance(event, MessageEvent) and event.source == "agent":
            return True
        if isinstance(event, ActionEvent):
            return False
    return False


def run_conversation_with_fake_user_response(
    conversation,
    fake_user_response_fn=fake_user_response,
    max_fake_responses: int = 10,
) -> None:
    """
    Run a conversation with automatic fake user responses.

    Args:
        conversation: The Conversation instance to run.
        fake_user_response_fn: Function that generates responses.
        max_fake_responses: Maximum responses before stopping (prevents infinite loops).
    """
    fake_response_count = 0

    while True:
        # Run the conversation
        conversation.run()

        status = conversation.state.execution_status

        # If not finished normally, stop
        if status != ConversationExecutionStatus.FINISHED:
            break

        # Check if agent used finish tool (proper completion)
        events = list(conversation.state.events)
        if _agent_finished_with_finish_action(events):
            break

        # Check if agent sent a message (needs fake response)
        if not _agent_sent_message(events):
            break

        # Check max responses limit
        if fake_response_count >= max_fake_responses:
            break

        # Send fake response and continue
        response = fake_user_response_fn(conversation)
        if response == "/exit":
            break

        conversation.send_message(response)
        fake_response_count += 1
```

</Accordion>

<Accordion title="Usage Example" icon="play">

Here's how to use this in your evaluation pipeline:

```python
from openhands.sdk import Agent, Conversation, LLM
from openhands.workspace import DockerWorkspace
from openhands.tools.preset.default import get_default_tools

# Set up your agent
llm = LLM(model="anthropic/claude-sonnet-4-20250514", api_key="...")
agent = Agent(llm=llm, tools=get_default_tools())

# Create workspace and conversation
workspace = DockerWorkspace()
conversation = Conversation(
    agent=agent,
    workspace=workspace,
    max_iteration_per_run=100,
)

# Send the task
conversation.send_message("Fix the bug in src/utils.py")

# Run with automatic fake user responses
run_conversation_with_fake_user_response(
    conversation,
    max_fake_responses=10,  # Prevent infinite loops
)

# Extract results from conversation.state.events
```

</Accordion>

<Accordion title="Alternative: Simple While Loop" icon="rotate">

For simpler cases, you can implement a basic while loop:

```python
max_retries = 10
retry_count = 0

conversation.send_message("Your task instruction here")

while retry_count < max_retries:
    conversation.run()

    events = list(conversation.state.events)

    # Check if agent finished properly
    for event in reversed(events):
        if isinstance(event, ActionEvent):
            if isinstance(event.action, FinishAction):
                break  # Done!
            break  # Used a tool, conversation ended for other reason

    # Check if agent sent a message
    last_agent_msg = None
    for event in reversed(events):
        if isinstance(event, MessageEvent) and event.source == "agent":
            last_agent_msg = event
            break
        if isinstance(event, ActionEvent):
            break

    if last_agent_msg is None:
        break  # No message to respond to

    # Send continuation prompt
    conversation.send_message(
        "Please continue. Use the finish tool when done."
    )
    retry_count += 1
```

</Accordion>

<Tip>
**Pro tip:** As a simpler alternative, you can add a hint to your task prompt:
> "If you're 100% done with the task, use the finish action. Otherwise, keep going until you're finished."

This encourages the agent to use the finish tool rather than asking for confirmation.
</Tip>

For the full implementation used in OpenHands benchmarks, see the [fake_user_response.py](https://github.com/OpenHands/benchmarks/blob/main/benchmarks/utils/fake_user_response.py) module.

## More questions?

If you have additional questions:

- **[Join our Slack Community](https://openhands.dev/joinslack)** - Ask questions and get help from the community
- **[GitHub Discussions](https://github.com/OpenHands/software-agent-sdk/discussions)** - Start a discussion
- **[GitHub Issues](https://github.com/OpenHands/software-agent-sdk/issues)** - Report bugs or request features
