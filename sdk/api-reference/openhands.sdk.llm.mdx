---
title: openhands.sdk.llm
description: API reference for openhands.sdk.llm module
---

## class CredentialStore

Store and retrieve OAuth credentials for LLM providers.

### Properties

- `credentials_dir`: Path
  Get the credentials directory, creating it if necessary.

### Methods

**__init__()**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/credentials.py#L49-L57)

Initialize the credential store.

**Parameters:**

- `credentials_dir` *Path | None* – Optional custom directory for storing credentials.
           Defaults to ~/.local/share/openhands/auth/


            <ParamField path="credentials_dir" type="Path | None" >
                None
            </ParamField>
            
**get() -> OAuthCredentials | None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/credentials.py#L72-L92)

Get stored credentials for a vendor.

**Parameters:**

- `vendor` *str* – The vendor/provider name (e.g., 'openai')

**Returns:**

- *OAuthCredentials | None* OAuthCredentials if found and valid, None otherwise


            <ParamField path="vendor" type="str" required>
                None
            </ParamField>
            
**save() -> None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/credentials.py#L94-L111)

Save credentials for a vendor.

**Parameters:**

- `credentials` *OAuthCredentials* – The OAuth credentials to save


            <ParamField path="credentials" type="OAuthCredentials" required>
                None
            </ParamField>
            
**delete() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/credentials.py#L113-L126)

Delete stored credentials for a vendor.

**Parameters:**

- `vendor` *str* – The vendor/provider name

**Returns:**

- *bool* True if credentials were deleted, False if they didn't exist


            <ParamField path="vendor" type="str" required>
                None
            </ParamField>
            
**update_tokens() -> OAuthCredentials | None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/credentials.py#L128-L157)

Update tokens for an existing credential.

**Parameters:**

- `vendor` *str* – The vendor/provider name
- `access_token` *str* – New access token
- `refresh_token` *str | None* – New refresh token (if provided)
- `expires_in` *int* – Token expiry in seconds

**Returns:**

- *OAuthCredentials | None* Updated credentials, or None if no existing credentials found


            <ParamField path="vendor" type="str" required>
                None
            </ParamField>
            

            <ParamField path="access_token" type="str" required>
                None
            </ParamField>
            

            <ParamField path="refresh_token" type="str | None" required>
                None
            </ParamField>
            

            <ParamField path="expires_in" type="int" required>
                None
            </ParamField>
            
## class OAuthCredentials

Bases: `BaseModel`

OAuth credentials for subscription-based LLM access.

### Properties

- `type`: Literal['oauth']
- `vendor`: str
  The vendor/provider (e.g., 
- `access_token`: str
  The OAuth access token
- `refresh_token`: str
  The OAuth refresh token
- `expires_at`: int
  Unix timestamp (ms) when the access token expires

### Methods

**is_expired() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/credentials.py#L39-L43)

Check if the access token is expired.

## class OpenAISubscriptionAuth

Handle OAuth authentication for OpenAI ChatGPT subscription access.

### Properties

- `vendor`: str
  Get the vendor name.

### Methods

**__init__()**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L345-L357)

Initialize the OpenAI subscription auth handler.

**Parameters:**

- `credential_store` *CredentialStore | None* – Optional custom credential store.
- `oauth_port` *int* – Port for the local OAuth callback server.


            <ParamField path="credential_store" type="CredentialStore | None" >
                None
            </ParamField>
            

            <ParamField path="oauth_port" type="int" >
                None
            </ParamField>
            
**get_credentials() -> OAuthCredentials | None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L364-L366)

Get stored credentials if they exist.

**has_valid_credentials() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L368-L371)

Check if valid (non-expired) credentials exist.

**refresh_if_needed() -> OAuthCredentials | None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L373-L397)

Refresh credentials if they are expired.

**Returns:**

- *OAuthCredentials | None* Updated credentials, or None if no credentials exist.

**Raises:**

- `RuntimeError` – If token refresh fails.

**login() -> OAuthCredentials**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L399-L527)

Perform OAuth login flow.

This starts a local HTTP server to handle the OAuth callback,
opens the browser for user authentication, and waits for the
callback with the authorization code.

**Parameters:**

- `open_browser` *bool* – Whether to automatically open the browser.

**Returns:**

- *OAuthCredentials* The obtained OAuth credentials.

**Raises:**

- `RuntimeError` – If the OAuth flow fails or times out.


            <ParamField path="open_browser" type="bool" >
                None
            </ParamField>
            
**logout() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L529-L535)

Remove stored credentials.

**Returns:**

- *bool* True if credentials were removed, False if none existed.

**create_llm() -> LLM**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/auth/openai.py#L537-L611)

Create an LLM instance configured for Codex subscription access.

**Parameters:**

- `model` *str* – The model to use (must be in OPENAI_CODEX_MODELS).
- `credentials` *OAuthCredentials | None* – OAuth credentials to use. If None, uses stored credentials.
- `instructions` *str | None* – Optional instructions for the Codex model.
- `**llm_kwargs` *Any* – Additional arguments to pass to LLM constructor.

**Returns:**

- *LLM* An LLM instance configured for Codex access.

**Raises:**

- `ValueError` – If the model is not supported or no credentials available.


            <ParamField path="model" type="str" >
                None
            </ParamField>
            

            <ParamField path="credentials" type="OAuthCredentials | None" >
                None
            </ParamField>
            

            <ParamField path="instructions" type="str | None" >
                None
            </ParamField>
            

            <ParamField path="llm_kwargs" type="Any" >
                None
            </ParamField>
            
## class LLM

Bases: `BaseModel`, `RetryMixin`, `NonNativeToolCallingMixin`

Language model interface for OpenHands agents.

The LLM class provides a unified interface for interacting with various
language models through the litellm library. It handles model configuration,
API authentication,
retry logic, and tool calling capabilities.

### Properties

- `model`: str
  Model name.
- `api_key`: str | SecretStr | None
  API key.
- `base_url`: str | None
  Custom base URL.
- `api_version`: str | None
  API version (e.g., Azure).
- `aws_access_key_id`: str | SecretStr | None
- `aws_secret_access_key`: str | SecretStr | None
- `aws_region_name`: str | None
- `openrouter_site_url`: str
- `openrouter_app_name`: str
- `num_retries`: int
- `retry_multiplier`: float
- `retry_min_wait`: int
- `retry_max_wait`: int
- `timeout`: int | None
  HTTP timeout in seconds. Default is 300s (5 minutes). Set to None to disable timeout (not recommended for production).
- `max_message_chars`: int
  Approx max chars in each event/content sent to the LLM.
- `temperature`: float | None
  Sampling temperature for response generation. Defaults to 0 for most models and provider default for reasoning models.
- `top_p`: float | None
- `top_k`: float | None
- `max_input_tokens`: int | None
  The maximum number of input tokens. Note that this is currently unused, and the value at runtime is actually the total tokens in OpenAI (e.g. 128,000 tokens for GPT-4).
- `max_output_tokens`: int | None
  The maximum number of output tokens. This is sent to the LLM.
- `model_canonical_name`: str | None
  Optional canonical model name for feature registry lookups. The OpenHands SDK maintains a model feature registry that maps model names to capabilities (e.g., vision support, prompt caching, responses API support). When using proxied or aliased model identifiers, set this field to the canonical model name (e.g., 
- `extra_headers`: dict[str, str] | None
  Optional HTTP headers to forward to LiteLLM requests.
- `input_cost_per_token`: float | None
  The cost per input token. This will available in logs for user.
- `output_cost_per_token`: float | None
  The cost per output token. This will available in logs for user.
- `ollama_base_url`: str | None
- `stream`: bool
  Enable streaming responses from the LLM. When enabled, the provided `on_token` callback in .completions and .responses will be invoked for each chunk of tokens.
- `drop_params`: bool
- `modify_params`: bool
  Modify params allows litellm to do transformations like adding a default message, when a message is empty.
- `disable_vision`: bool | None
  If model is vision capable, this option allows to disable image processing (useful for cost reduction).
- `disable_stop_word`: bool | None
  Disable using of stop word.
- `caching_prompt`: bool
  Enable caching of prompts.
- `log_completions`: bool
  Enable logging of completions.
- `log_completions_folder`: str
  The folder to log LLM completions to. Required if log_completions is True.
- `custom_tokenizer`: str | None
  A custom tokenizer to use for token counting.
- `native_tool_calling`: bool
  Whether to use native tool calling.
- `force_string_serializer`: bool | None
  Force using string content serializer when sending to LLM API. If None (default), auto-detect based on model. Useful for providers that do not support list content, like HuggingFace and Groq.
- `reasoning_effort`: Literal['low', 'medium', 'high', 'xhigh', 'none'] | None
  The effort to put into reasoning. This is a string that can be one of 
- `reasoning_summary`: Literal['auto', 'concise', 'detailed'] | None
  The level of detail for reasoning summaries. This is a string that can be one of 
- `enable_encrypted_reasoning`: bool
  If True, ask for [
- `prompt_cache_retention`: str | None
  Retention policy for prompt cache. Only sent for GPT-5+ models; explicitly stripped for all other models.
- `extended_thinking_budget`: int | None
  The budget tokens for extended thinking, supported by Anthropic models.
- `seed`: int | None
  The seed to use for random number generation.
- `safety_settings`: list[dict[str, str]] | None
  Deprecated: Safety settings for models that support them (like Mistral AI and Gemini). This field is deprecated in 1.10.0 and will be removed in 1.15.0. Safety settings are designed for consumer-facing content moderation, which is not relevant for coding agents.
- `usage_id`: str
  Unique usage identifier for the LLM. Used for registry lookups, telemetry, and spend tracking.
- `litellm_extra_body`: dict[str, Any]
  Additional key-value pairs to pass to litellm
- `retry_listener`: SkipJsonSchema[Callable[[int, int, BaseException | None], None] | None]
- `model_config`: ConfigDict
- `metrics`: Metrics
  Get usage metrics for this LLM instance.

Returns:
    Metrics object containing token usage, costs, and other statistics.

Example:
    >>> cost = llm.metrics.accumulated_cost
    >>> print(f"Total cost: $\{cost\}")
- `telemetry`: Telemetry
  Get telemetry handler for this LLM instance.

Returns:
    Telemetry object for managing logging and metrics callbacks.

Example:
    >>> llm.telemetry.set_log_completions_callback(my_callback)
- `is_subscription`: bool
  Check if this LLM uses subscription-based authentication.

Returns True when the LLM was created via `LLM.subscription_login()`,
which uses the ChatGPT subscription Codex backend rather than the
standard OpenAI API.

Returns:
    bool: True if using subscription-based transport, False otherwise.
- `model_info`: dict | None
  Returns the model info dictionary.

### Methods

**restore_metrics() -> None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L531-L533)


            <ParamField path="metrics" type="Metrics" required>
                None
            </ParamField>
            
**completion() -> LLMResponse**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L535-L691)

Generate a completion from the language model.

This is the method for getting responses from the model via Completion API.
It handles message formatting, tool calling, and response processing.

**Parameters:**

- `messages` *list[Message]* – List of conversation messages
- `tools` *Sequence[ToolDefinition] | None* – Optional list of tools available to the model
- `_return_metrics` *bool* – Whether to return usage metrics
- `add_security_risk_prediction` *bool* – Add security_risk field to tool schemas
- `on_token` *TokenCallbackType | None* – Optional callback for streaming tokens
- `**kwargs` – Additional arguments passed to the LLM API

**Returns:**

- *LLMResponse* LLMResponse containing the model's response and metadata.

**Raises:**

- `ValueError` – If streaming is requested (not supported).


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            

            <ParamField path="tools" type="Sequence[ToolDefinition] | None" >
                None
            </ParamField>
            

            <ParamField path="_return_metrics" type="bool" >
                None
            </ParamField>
            

            <ParamField path="add_security_risk_prediction" type="bool" >
                None
            </ParamField>
            

            <ParamField path="on_token" type="TokenCallbackType | None" >
                None
            </ParamField>
            

            <ParamField path="kwargs" type="" >
                None
            </ParamField>
            
**responses() -> LLMResponse**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L696-L892)

Alternative invocation path using OpenAI Responses API via LiteLLM.

Maps Message[] -> (instructions, input[]) and returns LLMResponse.

**Parameters:**

- `messages` *list[Message]* – List of conversation messages
- `tools` *Sequence[ToolDefinition] | None* – Optional list of tools available to the model
- `include` *list[str] | None* – Optional list of fields to include in response
- `store` *bool | None* – Whether to store the conversation
- `_return_metrics` *bool* – Whether to return usage metrics
- `add_security_risk_prediction` *bool* – Add security_risk field to tool schemas
- `on_token` *TokenCallbackType | None* – Optional callback for streaming deltas
- `**kwargs` – Additional arguments passed to the API


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            

            <ParamField path="tools" type="Sequence[ToolDefinition] | None" >
                None
            </ParamField>
            

            <ParamField path="include" type="list[str] | None" >
                None
            </ParamField>
            

            <ParamField path="store" type="bool | None" >
                None
            </ParamField>
            

            <ParamField path="_return_metrics" type="bool" >
                None
            </ParamField>
            

            <ParamField path="add_security_risk_prediction" type="bool" >
                None
            </ParamField>
            

            <ParamField path="on_token" type="TokenCallbackType | None" >
                None
            </ParamField>
            

            <ParamField path="kwargs" type="" >
                None
            </ParamField>
            
**vision_is_active() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1024-L1027)

**is_caching_prompt_active() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1052-L1066)

Check if prompt caching is supported and enabled for current model.

**Returns:**

- *bool* True if prompt caching is supported and enabled for the given
model.

**uses_responses_api() -> bool**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1068-L1072)

Whether this model uses the OpenAI Responses API path.

**format_messages_for_llm() -> list[dict]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1098-L1127)

Formats Message objects for LLM consumption.


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            
**format_messages_for_responses() -> tuple[str | None, list[dict[str, Any]]]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1129-L1168)

Prepare (instructions, input[]) for the OpenAI Responses API.

- Skips prompt caching flags and string serializer concerns
- Uses Message.to_responses_value to get either instructions (system)
  or input items (others)
- Concatenates system instructions into a single instructions string
- For subscription mode, system prompts are prepended to user content


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            
**get_token_count() -> int**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1170-L1193)


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            
**load_from_json() -> LLM**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1198-L1202)


            <ParamField path="json_path" type="str" required>
                None
            </ParamField>
            
**load_from_env() -> LLM**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1204-L1257)


            <ParamField path="prefix" type="str" >
                None
            </ParamField>
            
**subscription_login() -> LLM**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm.py#L1259-L1316)

Authenticate with a subscription service and return an LLM instance.

This method provides subscription-based access to LLM models that are
available through chat subscriptions (e.g., ChatGPT Plus/Pro) rather
than API credits. It handles credential caching, token refresh, and
the OAuth login flow.

Currently supported vendors:
- "openai": ChatGPT Plus/Pro subscription for Codex models

Supported OpenAI models:
- gpt-5.1-codex-max
- gpt-5.1-codex-mini
- gpt-5.2
- gpt-5.2-codex

**Parameters:**

- `vendor` *SupportedVendor* – The vendor/provider. Currently only "openai" is supported.
- `model` *str* – The model to use. Must be supported by the vendor's
subscription service.
- `force_login` *bool* – If True, always perform a fresh login even if valid
credentials exist.
- `open_browser` *bool* – Whether to automatically open the browser for the
OAuth login flow.
- `**llm_kwargs` – Additional arguments to pass to the LLM constructor.

**Returns:**

- *LLM* An LLM instance configured for subscription-based access.

**Raises:**

- `ValueError` – If the vendor or model is not supported.
- `RuntimeError` – If authentication fails.


            <ParamField path="vendor" type="SupportedVendor" required>
                None
            </ParamField>
            

            <ParamField path="model" type="str" required>
                None
            </ParamField>
            

            <ParamField path="force_login" type="bool" >
                None
            </ParamField>
            

            <ParamField path="open_browser" type="bool" >
                None
            </ParamField>
            

            <ParamField path="llm_kwargs" type="" >
                None
            </ParamField>
            
## class LLMRegistry

A minimal LLM registry for managing LLM instances by usage ID.

This registry provides a simple way to manage multiple LLM instances,
avoiding the need to recreate LLMs with the same configuration.

### Properties

- `registry_id`: str
- `retry_listener`: Callable[[int, int], None] | None
- `subscriber`: Callable[[RegistryEvent], None] | None
- `usage_to_llm`: MappingProxyType[str, LLM]
  Access the internal usage-ID-to-LLM mapping (read-only view).

### Methods

**__init__()**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm_registry.py#L33-L45)

Initialize the LLM registry.

**Parameters:**

- `retry_listener` *Callable[[int, int], None] | None* – Optional callback for retry events.


            <ParamField path="retry_listener" type="Callable[[int, int], None] | None" >
                None
            </ParamField>
            
**subscribe() -> None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm_registry.py#L47-L53)

Subscribe to registry events.

**Parameters:**

- `callback` *Callable[[RegistryEvent], None]* – Function to call when LLMs are created or updated.


            <ParamField path="callback" type="Callable[[RegistryEvent], None]" required>
                None
            </ParamField>
            
**notify() -> None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm_registry.py#L55-L65)

Notify subscribers of registry events.

**Parameters:**

- `event` *RegistryEvent* – The registry event to notify about.


            <ParamField path="event" type="RegistryEvent" required>
                None
            </ParamField>
            
**add() -> None**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm_registry.py#L73-L95)

Add an LLM instance to the registry.

**Parameters:**

- `llm` *LLM* – The LLM instance to register.

**Raises:**

- `ValueError` – If llm.usage_id already exists in the registry.


            <ParamField path="llm" type="LLM" required>
                None
            </ParamField>
            
**get() -> LLM**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm_registry.py#L97-L118)

Get an LLM instance from the registry.

**Parameters:**

- `usage_id` *str* – Unique identifier for the LLM usage slot.

**Returns:**

- *LLM* The LLM instance.

**Raises:**

- `KeyError` – If usage_id is not found in the registry.


            <ParamField path="usage_id" type="str" required>
                None
            </ParamField>
            
**list_usage_ids() -> list[str]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/llm_registry.py#L120-L123)

List all registered usage IDs.

## class RegistryEvent

Bases: `BaseModel`

### Properties

- `llm`: LLM
- `model_config`: ConfigDict

## class LLMResponse

Bases: `BaseModel`

Result of an LLM completion request.

This type provides a clean interface for LLM completion results, exposing
only OpenHands-native types to consumers while preserving access to the
raw LiteLLM response for internal use.

### Properties

- `message`: Message
- `metrics`: MetricsSnapshot
- `raw_response`: ModelResponse | ResponsesAPIResponse
- `model_config`: ConfigDict
- `id`: str
  Get the response ID from the underlying LLM response.

This property provides a clean interface to access the response ID,
supporting both completion mode (ModelResponse) and response API modes
(ResponsesAPIResponse).

Returns:
    The response ID from the LLM response

## class ImageContent

Bases: `BaseContent`

### Properties

- `type`: Literal['image']
- `image_urls`: list[str]

### Methods

**to_llm_dict() -> list[dict[str, str | dict[str, str]]]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L189-L196)

Convert to LLM API format.

## class Message

Bases: `BaseModel`

### Properties

- `role`: Literal['user', 'system', 'assistant', 'tool']
- `content`: Sequence[TextContent | ImageContent]
- `tool_calls`: list[MessageToolCall] | None
- `tool_call_id`: str | None
- `name`: str | None
- `reasoning_content`: str | None
  Intermediate reasoning/thinking content from reasoning models
- `thinking_blocks`: Sequence[ThinkingBlock | RedactedThinkingBlock]
  Raw Anthropic thinking blocks for extended thinking feature
- `responses_reasoning_item`: ReasoningItemModel | None
  OpenAI Responses reasoning item from model output
- `model_config`
- `contains_image`: bool

### Methods

**to_chat_dict() -> dict[str, Any]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L281-L329)

Serialize message for OpenAI Chat Completions.

**Parameters:**

- `cache_enabled` *bool* – Whether prompt caching is active.
- `vision_enabled` *bool* – Whether vision/image processing is enabled.
- `function_calling_enabled` *bool* – Whether native function calling is enabled.
- `force_string_serializer` *bool* – Force string serializer instead of list format.
- `send_reasoning_content` *bool* – Whether to include reasoning_content in output.

Chooses the appropriate content serializer and then injects threading keys:
- Assistant tool call turn: role == "assistant" and self.tool_calls
- Tool result turn: role == "tool" and self.tool_call_id (with name)


            <ParamField path="cache_enabled" type="bool" required>
                None
            </ParamField>
            

            <ParamField path="vision_enabled" type="bool" required>
                None
            </ParamField>
            

            <ParamField path="function_calling_enabled" type="bool" required>
                None
            </ParamField>
            

            <ParamField path="force_string_serializer" type="bool" required>
                None
            </ParamField>
            

            <ParamField path="send_reasoning_content" type="bool" required>
                None
            </ParamField>
            
**to_responses_value() -> str | list[dict[str, Any]]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L439-L449)

Return serialized form.

Either an instructions string (for system) or input items (for other roles).


            <ParamField path="vision_enabled" type="bool" required>
                None
            </ParamField>
            
**to_responses_dict() -> list[dict[str, Any]]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L451-L578)

Serialize message for OpenAI Responses (input parameter).

Produces a list of "input" items for the Responses API:
- system: returns [], system content is expected in 'instructions'
- user: one 'message' item with content parts -> input_text / input_image
(when vision enabled)
- assistant: emits prior assistant content as input_text,
and function_call items for tool_calls
- tool: emits function_call_output items (one per TextContent)
with matching call_id


            <ParamField path="vision_enabled" type="bool" required>
                None
            </ParamField>
            
**from_llm_chat_message() -> Message**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L590-L647)

Convert a LiteLLMMessage (Chat Completions) to our Message class.

Provider-agnostic mapping for reasoning:
- Prefer `message.reasoning_content` if present (LiteLLM normalized field)
- Extract `thinking_blocks` from content array (Anthropic-specific)


            <ParamField path="message" type="LiteLLMMessage" required>
                None
            </ParamField>
            
**from_llm_responses_output() -> Message**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L649-L705)

Convert OpenAI Responses API output items into a single assistant Message.

Policy (non-stream):
- Collect assistant text by concatenating output_text parts from message items
- Normalize function_call items to MessageToolCall list


            <ParamField path="output" type="Any" required>
                None
            </ParamField>
            
## class MessageToolCall

Bases: `BaseModel`

Transport-agnostic tool call representation.

One canonical id is used for linking across actions/observations and
for Responses function_call_output call_id.

### Properties

- `id`: str
  Canonical tool call id
- `name`: str
  Tool/function name
- `arguments`: str
  JSON string of arguments
- `origin`: Literal['completion', 'responses']
  Originating API family

### Methods

**from_chat_tool_call() -> MessageToolCall**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L38-L58)

Create a MessageToolCall from a Chat Completions tool call.


            <ParamField path="tool_call" type="ChatCompletionMessageToolCall" required>
                None
            </ParamField>
            
**from_responses_function_call() -> MessageToolCall**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L60-L82)

Create a MessageToolCall from a typed OpenAI Responses function_call item.

Note: OpenAI Responses function_call.arguments is already a JSON string.


            <ParamField path="item" type="ResponseFunctionToolCall | OutputFunctionToolCall" required>
                None
            </ParamField>
            
**to_chat_dict() -> dict[str, Any]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L84-L93)

Serialize to OpenAI Chat Completions tool_calls format.

**to_responses_dict() -> dict[str, Any]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L95-L111)

Serialize to OpenAI Responses 'function_call' input item format.

## class ReasoningItemModel

Bases: `BaseModel`

OpenAI Responses reasoning item (non-stream, subset we consume).

Do not log or render encrypted_content.

### Properties

- `id`: str | None
- `summary`: list[str]
- `content`: list[str] | None
- `encrypted_content`: str | None
- `status`: str | None

## class RedactedThinkingBlock

Bases: `BaseModel`

Redacted thinking block for previous responses without extended thinking.

This is used as a placeholder for assistant messages that were generated
before extended thinking was enabled.

### Properties

- `type`: Literal['redacted_thinking']
- `data`: str
  The redacted thinking content

## class TextContent

Bases: `BaseContent`

### Properties

- `type`: Literal['text']
- `text`: str
- `model_config`: ConfigDict

### Methods

**to_llm_dict() -> list[dict[str, str | dict[str, str]]]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L174-L182)

Convert to LLM API format.

## class ThinkingBlock

Bases: `BaseModel`

Anthropic thinking block for extended thinking feature.

This represents the raw thinking blocks returned by Anthropic models
when extended thinking is enabled. These blocks must be preserved
and passed back to the API for tool use scenarios.

### Properties

- `type`: Literal['thinking']
- `thinking`: str
  The thinking content
- `signature`: str | None
  Cryptographic signature for the thinking block

**content_to_str() -> list[str]**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/message.py#L708-L719)

Convert a list of TextContent and ImageContent to a list of strings.

This is primarily used for display purposes.


            <ParamField path="contents" type="Sequence[TextContent | ImageContent]" required>
                None
            </ParamField>
            
## class RouterLLM

Bases: `LLM`

Base class for multiple LLM acting as a unified LLM.
This class provides a foundation for implementing model routing by
inheriting from LLM, allowing routers to work with multiple underlying
LLM models while presenting a unified LLM interface to consumers.
Key features:
- Works with multiple LLMs configured via llms_for_routing
- Delegates all other operations/properties to the selected LLM
- Provides routing interface through select_llm() method

### Properties

- `router_name`: str
  Name of the router
- `llms_for_routing`: dict[str, LLM]
- `active_llm`: LLM | None
  Currently selected LLM instance

### Methods

**validate_llms_not_empty()**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/router/base.py#L41-L48)


            <ParamField path="v" type="" required>
                None
            </ParamField>
            
**completion() -> LLMResponse**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/router/base.py#L50-L89)

This method intercepts completion calls and routes them to the appropriate
underlying LLM based on the routing logic implemented in select_llm().

**Parameters:**

- `messages` *list[Message]* – List of conversation messages
- `tools` *Sequence[ToolDefinition] | None* – Optional list of tools available to the model
- `return_metrics` *bool* – Whether to return usage metrics
- `add_security_risk_prediction` *bool* – Add security_risk field to tool schemas
- `on_token` *TokenCallbackType | None* – Optional callback for streaming tokens
- `**kwargs` – Additional arguments passed to the LLM API


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            

            <ParamField path="tools" type="Sequence[ToolDefinition] | None" >
                None
            </ParamField>
            

            <ParamField path="return_metrics" type="bool" >
                None
            </ParamField>
            

            <ParamField path="add_security_risk_prediction" type="bool" >
                None
            </ParamField>
            

            <ParamField path="on_token" type="TokenCallbackType | None" >
                None
            </ParamField>
            

            <ParamField path="kwargs" type="" >
                None
            </ParamField>
            
**abstractmethod select_llm() -> str**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/router/base.py#L91-L105)

Select which LLM to use based on messages and events.

This method implements the core routing logic for the RouterLLM.
Subclasses should analyze the provided messages to determine which
LLM from llms_for_routing is most appropriate for handling the request.

**Parameters:**

- `messages` *list[Message]* – List of messages in the conversation that can be used
     to inform the routing decision.

**Returns:**

- *str* The key/name of the LLM to use from llms_for_routing dictionary.


            <ParamField path="messages" type="list[Message]" required>
                None
            </ParamField>
            
**set_placeholder_model()**

[source](https://github.com/OpenHands/software-agent-sdk/blob/30e22095607254194ab1f4d8859f658abe5628a1/openhands-sdk/openhands/sdk/llm/router/base.py#L117-L129)

Guarantee `model` exists before LLM base validation runs.


            <ParamField path="data" type="" required>
                None
            </ParamField>
            