---
title: LLM configuration details
description: Configure LLM objects in the Agent SDK and reuse them across OpenHands interfaces.
---

This page expands on how to configure `openhands.sdk.llm.LLM` instances. The
same options back every interface built on top of the Agent SDK—including the
OpenHands UI—so documenting them here keeps the configuration story consistent.

## Environment variable layout

The SDK expects environment variables prefixed with `LLM_`. They are lowercased
and mapped onto field names when you call `LLM.load_from_env()`.

```bash
export LLM_MODEL="anthropic/claude-sonnet-4.1"
export LLM_API_KEY="sk-ant-123"
export LLM_SERVICE_ID="primary"
export LLM_TIMEOUT="120"
export LLM_NUM_RETRIES="5"
```

Then, in Python:

```python
from openhands.sdk import LLM

llm = LLM.load_from_env()
```

The loader automatically casts values into integers, floats, booleans, JSON, or
`SecretStr` where appropriate. Unknown variables are ignored, so it is safe to
store extra settings alongside LLM keys in `.env` files or secret managers.

## JSON configuration

For declarative deployments you can persist the SDK model schema to JSON:

```python
from pydantic import SecretStr

llm = LLM(
    model="openai/gpt-4o",
    api_key=SecretStr("sk-openai"),
    temperature=0.1,
)
with open("config/llm.json", "w") as fp:
    fp.write(llm.model_dump_json(exclude_none=True, indent=2))

reloaded = LLM.load_from_json("config/llm.json")
```

Serialized structures redact secrets (API keys, AWS credentials). Combine the
JSON file with environment variables for secrets when your runtime requires
human review of committed configuration.

## Commonly tuned parameters

- **Latency & retry controls**: `timeout`, `num_retries`, `retry_min_wait`,
  `retry_max_wait`, and `retry_multiplier` govern LiteLLM retry behavior across
  providers.
- **Prompt shaping**: `temperature`, `top_p`, `top_k`, `reasoning_effort`, and
  `extended_thinking_budget` adjust sampling characteristics and Anthropic
  reasoning budgets.
- **Cost reporting**: `input_cost_per_token` and `output_cost_per_token` flow
  into SDK telemetry so downstream interfaces can display usage estimates.
- **Provider specific knobs**: fields such as `api_version` (Azure),
  `ollama_base_url`, `custom_llm_provider`, and `openrouter_site_url` allow
  compatibility with LiteLLM's provider adapters without custom glue code.

Refer to the docstring within
[`openhands.sdk.llm.LLM`](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands-sdk/openhands/sdk/llm/llm.py)
for the full schema. All fields can be set programmatically or via environment
variables using the naming rule `field -> LLM_FIELD`.

## Multiple profiles

Many applications require separate model profiles (for example, a premium coding
model and a cheaper summarizer). Pair environment prefixes with registry
service IDs:

```bash
export CODER_LLM_MODEL="anthropic/claude-sonnet-4.1"
export CODER_LLM_API_KEY="sk-ant"
export SUMMARIZER_LLM_MODEL="openai/gpt-4o-mini"
export SUMMARIZER_LLM_API_KEY="sk-openai"
```

```python
coder = LLM.load_from_env(prefix="CODER_LLM_").model_copy(update={"service_id": "coder"})
summarizer = LLM.load_from_env(prefix="SUMMARIZER_LLM_").model_copy(update={"service_id": "summarizer"})
```

These instances can then be registered with `LLMRegistry` and shared across
agents or microservices. The OpenHands UI uses the same pattern when end users
create named profiles.
