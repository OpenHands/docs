---
title: LLM Integration
description: Language model integration supporting multiple providers through LiteLLM with built-in retry logic and metrics tracking.
---

The LLM class provides a unified interface for language model integration, supporting multiple providers through [LiteLLM](https://docs.litellm.ai/). It handles authentication, retries, metrics tracking, and streaming responses.

**Source**: [`openhands/sdk/llm/`](https://github.com/All-Hands-AI/agent-sdk/tree/main/openhands/sdk/llm)

## Core Concepts

```mermaid
graph LR
    LLM[LLM] --> Completion[completion()]
    LLM --> Metrics[Metrics Tracking]
    LLM --> Retry[Retry Logic]
    
    Completion --> Provider[Provider API]
    Provider --> OpenAI[OpenAI]
    Provider --> Anthropic[Anthropic]
    Provider --> Others[Other Providers]
    
    style LLM fill:#e1f5fe
    style Completion fill:#fff3e0
    style Metrics fill:#e8f5e8
    style Retry fill:#f3e5f5
```

## Basic Usage

**Source**: [`openhands/sdk/llm/llm.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands/sdk/llm/llm.py)

### Creating an LLM

```python
from openhands.sdk import LLM
from pydantic import SecretStr

# Basic configuration
llm = LLM(
    model="anthropic/claude-sonnet-4-20250514",
    api_key=SecretStr("your-api-key")
)

# With custom settings
llm = LLM(
    model="openai/gpt-4",
    api_key=SecretStr("your-api-key"),
    base_url="https://api.openai.com/v1",
    temperature=0.7,
    max_tokens=4096,
    timeout=60.0
)
```

### Configuration Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model` | `str` | `"claude-sonnet-4-20250514"` | Model identifier |
| `api_key` | `SecretStr \| None` | `None` | API key for authentication |
| `base_url` | `str \| None` | `None` | Custom API endpoint |
| `temperature` | `float` | `0.0` | Sampling temperature (0-2) |
| `max_tokens` | `int \| None` | `None` | Maximum tokens to generate |
| `timeout` | `float` | `60.0` | Request timeout in seconds |
| `num_retries` | `int` | `8` | Number of retry attempts |
| `retry_min_wait` | `int` | `3` | Minimum retry wait (seconds) |
| `retry_max_wait` | `int` | `60` | Maximum retry wait (seconds) |
| `retry_multiplier` | `float` | `2.0` | Retry backoff multiplier |

## Generating Completions

### Basic Completion

```python
from openhands.sdk.llm import Message

messages = [
    Message(role="user", content="What is the capital of France?")
]

response = llm.completion(messages=messages)
print(response.choices[0].message.content)
# Output: "The capital of France is Paris."
```

### With Tool Calling

```python
from openhands.sdk import Agent
from openhands.tools import BashTool

# Tools are automatically converted to function schemas
agent = Agent(
    llm=llm,
    tools=[BashTool.create()]
)

# LLM receives tool schemas and can call them
```

### Streaming Responses

```python
# Enable streaming
llm = LLM(
    model="anthropic/claude-sonnet-4-20250514",
    api_key=SecretStr("your-api-key"),
    stream=True
)

# Stream response chunks
for chunk in llm.completion(messages=messages):
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

## Model Providers

The SDK supports all providers available in LiteLLM:

### Anthropic

```python
llm = LLM(
    model="anthropic/claude-sonnet-4-20250514",
    api_key=SecretStr("sk-ant-...")
)
```

### OpenAI

```python
llm = LLM(
    model="openai/gpt-4",
    api_key=SecretStr("sk-...")
)
```

### Azure OpenAI

```python
llm = LLM(
    model="azure/gpt-4",
    api_key=SecretStr("your-azure-key"),
    api_base="https://your-resource.openai.azure.com",
    api_version="2024-02-01"
)
```

### Custom Providers

```python
llm = LLM(
    model="custom-provider/model-name",
    base_url="https://custom-api.example.com/v1",
    api_key=SecretStr("your-api-key")
)
```

See [LiteLLM providers](https://docs.litellm.ai/docs/providers) for full list.

## LLM Registry

**Source**: Use pre-configured LLM instances from registry.

See [`examples/01_standalone_sdk/05_use_llm_registry.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/examples/01_standalone_sdk/05_use_llm_registry.py):

```python
from openhands.sdk.llm.registry import get_llm

# Get pre-configured LLM
llm = get_llm(
    model_name="claude-sonnet-4",
    # Configuration from environment or defaults
)
```

## Metrics and Monitoring

### Tracking Metrics

**Source**: [`openhands/sdk/llm/utils/metrics.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands/sdk/llm/utils/metrics.py)

```python
# Get metrics snapshot
metrics = llm.metrics.snapshot()

print(f"Total tokens: {metrics.accumulated_cost}")
print(f"Total cost: ${metrics.accumulated_cost}")
print(f"Requests: {metrics.total_requests}")
```

See [`examples/01_standalone_sdk/13_get_llm_metrics.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/examples/01_standalone_sdk/13_get_llm_metrics.py).

### Cost Tracking

```python
from openhands.sdk.conversation import Conversation

conversation = Conversation(agent=Agent(llm=llm, tools=tools))
conversation.send_message("Task")
conversation.run()

# Get conversation stats
stats = conversation.conversation_stats
print(f"Total tokens: {stats.total_tokens}")
print(f"Estimated cost: ${stats.total_cost}")
```

## Advanced Features

### LLM Routing

**Source**: Route between different LLMs based on criteria.

See [`examples/01_standalone_sdk/19_llm_routing.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/examples/01_standalone_sdk/19_llm_routing.py):

```python
# Use different LLMs for different tasks
fast_llm = LLM(model="openai/gpt-4o-mini", api_key=SecretStr("..."))
powerful_llm = LLM(model="anthropic/claude-sonnet-4-20250514", api_key=SecretStr("..."))

# Route based on task complexity
if task_is_simple:
    agent = Agent(llm=fast_llm, tools=tools)
else:
    agent = Agent(llm=powerful_llm, tools=tools)
```

### Model Reasoning

**Source**: Access model reasoning from Anthropic thinking blocks and OpenAI responses API.

See [`examples/01_standalone_sdk/22_model_reasoning.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/examples/01_standalone_sdk/22_model_reasoning.py):

```python
# Enable Anthropic extended thinking
llm = LLM(
    model="anthropic/claude-sonnet-4-20250514",
    api_key=SecretStr("your-api-key"),
    thinking={"type": "enabled", "budget_tokens": 1000}
)

# Or use OpenAI responses API for reasoning
llm = LLM(
    model="openai/gpt-5-codex",
    api_key=SecretStr("your-api-key"),
    reasoning_effort="high"
)
```

## Error Handling

### Automatic Retries

The LLM class automatically retries on transient failures:

```python
from litellm.exceptions import RateLimitError, APIConnectionError

# These exceptions trigger automatic retry:
# - APIConnectionError
# - RateLimitError  
# - ServiceUnavailableError
# - Timeout
# - InternalServerError

# Configure retry behavior
llm = LLM(
    model="anthropic/claude-sonnet-4-20250514",
    api_key=SecretStr("your-api-key"),
    num_retries=8,  # Number of retries
    retry_min_wait=3,  # Min wait between retries (seconds)
    retry_max_wait=60,  # Max wait between retries (seconds)
    retry_multiplier=2.0  # Exponential backoff multiplier
)
```

### Exception Handling

```python
from litellm.exceptions import (
    RateLimitError,
    ContextWindowExceededError,
    BadRequestError
)

try:
    response = llm.completion(messages=messages)
except RateLimitError:
    print("Rate limit exceeded, automatic retry in progress")
except ContextWindowExceededError:
    print("Context window exceeded, reduce message history")
except BadRequestError as e:
    print(f"Bad request: {e}")
```

## Message Types

**Source**: [`openhands/sdk/llm/message.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands/sdk/llm/message.py)

### Text Messages

```python
from openhands.sdk.llm import Message

message = Message(
    role="user",
    content="Hello, how are you?"
)
```

### Multimodal Messages

```python
from openhands.sdk.llm import Message, ImageContent

message = Message(
    role="user",
    content=[
        "What's in this image?",
        ImageContent(source="path/to/image.png")
    ]
)
```

See [`examples/01_standalone_sdk/17_image_input.py`](https://github.com/All-Hands-AI/agent-sdk/blob/main/examples/01_standalone_sdk/17_image_input.py).

### Tool Call Messages

```python
from openhands.sdk.llm import Message, MessageToolCall

# Message with tool calls
message = Message(
    role="assistant",
    content="Let me run that command",
    tool_calls=[
        MessageToolCall(
            id="call_123",
            function={"name": "execute_bash", "arguments": '{"command": "ls"}'}
        )
    ]
)
```

## Model Features

### Vision Support

```python
from litellm.utils import supports_vision

if supports_vision(llm.model):
    # Model supports image inputs
    message = Message(
        role="user",
        content=["Describe this image", ImageContent(source="image.png")]
    )
```

### Token Counting

```python
from litellm.utils import token_counter

# Count tokens in messages
messages = [Message(role="user", content="Hello world")]
tokens = token_counter(model=llm.model, messages=messages)
print(f"Message uses {tokens} tokens")
```

### Model Information

```python
from litellm.utils import get_model_info

info = get_model_info(llm.model)
print(f"Max tokens: {info['max_tokens']}")
print(f"Cost per token: {info['input_cost_per_token']}")
```

## Best Practices

1. **Set Appropriate Timeouts**: Adjust based on expected response time
2. **Configure Retries**: Balance reliability with latency requirements
3. **Monitor Metrics**: Track token usage and costs
4. **Handle Exceptions**: Implement proper error handling
5. **Use Streaming**: For better user experience with long responses
6. **Secure API Keys**: Use `SecretStr` and environment variables
7. **Choose Right Model**: Balance cost, speed, and capability

## Environment Variables

Configure LLM via environment variables:

```bash
# API keys
export ANTHROPIC_API_KEY="sk-ant-..."
export OPENAI_API_KEY="sk-..."
export AZURE_API_KEY="..."

# Custom endpoints
export OPENAI_API_BASE="https://custom-endpoint.com"

# Model defaults
export LLM_MODEL="anthropic/claude-sonnet-4-20250514"
```

## See Also

- **[Agent](/sdk/architecture/sdk/agent.mdx)** - Using LLMs with agents
- **[Message Types](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands/sdk/llm/message.py)** - Message structure
- **[LiteLLM Documentation](https://docs.litellm.ai/)** - Provider details
- **[Examples](https://github.com/All-Hands-AI/agent-sdk/tree/main/examples/01_standalone_sdk)** - LLM usage examples
