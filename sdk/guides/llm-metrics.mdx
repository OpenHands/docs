---
title: Metrics Tracking
description: Track token usage, costs, and performance metrics for your agents.
---

<Note>
This example is available on GitHub: [examples/01_standalone_sdk/13_get_llm_metrics.py](https://github.com/All-Hands-AI/agent-sdk/blob/main/examples/01_standalone_sdk/13_get_llm_metrics.py)
</Note>

Track token usage, costs, and performance metrics from LLM interactions:

```python icon="python" expandable examples/01_standalone_sdk/13_get_llm_metrics.py
import os

from pydantic import SecretStr

from openhands.sdk import (
    LLM,
    Agent,
    Conversation,
    Event,
    LLMConvertibleEvent,
    get_logger,
)
from openhands.sdk.tool import Tool, register_tool
from openhands.tools.execute_bash import BashTool
from openhands.tools.file_editor import FileEditorTool


logger = get_logger(__name__)

# Configure LLM
api_key = os.getenv("LLM_API_KEY")
assert api_key is not None, "LLM_API_KEY environment variable is not set."
model = os.getenv("LLM_MODEL", "openhands/claude-sonnet-4-5-20250929")
base_url = os.getenv("LLM_BASE_URL")
llm = LLM(
    usage_id="agent",
    model=model,
    base_url=base_url,
    api_key=SecretStr(api_key),
)

cwd = os.getcwd()
register_tool("BashTool", BashTool)
register_tool("FileEditorTool", FileEditorTool)
tools = [
    Tool(name="BashTool"),
    Tool(name="FileEditorTool"),
]

# Add MCP Tools
mcp_config = {"mcpServers": {"fetch": {"command": "uvx", "args": ["mcp-server-fetch"]}}}

# Agent
agent = Agent(llm=llm, tools=tools, mcp_config=mcp_config)

llm_messages = []  # collect raw LLM messages


def conversation_callback(event: Event):
    if isinstance(event, LLMConvertibleEvent):
        llm_messages.append(event.to_llm_message())


# Conversation
conversation = Conversation(
    agent=agent,
    callbacks=[conversation_callback],
    workspace=cwd,
)

logger.info("Starting conversation with MCP integration...")
conversation.send_message(
    "Read https://github.com/OpenHands/OpenHands and write 3 facts "
    "about the project into FACTS.txt."
)
conversation.run()

conversation.send_message("Great! Now delete that file.")
conversation.run()

print("=" * 100)
print("Conversation finished. Got the following LLM messages:")
for i, message in enumerate(llm_messages):
    print(f"Message {i}: {str(message)[:200]}")

assert llm.metrics is not None
print(
    f"Conversation finished. Final LLM metrics with details: {llm.metrics.model_dump()}"
)
```

```bash Running the Example
export LLM_API_KEY="your-api-key"
cd agent-sdk
uv run python examples/01_standalone_sdk/13_get_llm_metrics.py
```

### Getting Metrics

Access metrics directly from the LLM object after running the conversation:

```python highlight={3-4}
conversation.run()

assert llm.metrics is not None
print(f"Final LLM metrics: {llm.metrics.model_dump()}")
```

The `llm.metrics` object is an instance of the [Metrics class](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands-sdk/openhands/sdk/llm/utils/metrics.py), which provides detailed information including:

- `accumulated_cost` - Total accumulated cost across all API calls
- `accumulated_token_usage` - Aggregated token usage with fields like:
  - `prompt_tokens` - Number of input tokens processed
  - `completion_tokens` - Number of output tokens generated
  - `cache_read_tokens` - Cache hits (if supported by the model)
  - `cache_write_tokens` - Cache writes (if supported by the model)
  - `reasoning_tokens` - Reasoning tokens (for models that support extended thinking)
  - `context_window` - Context window size used
- `costs` - List of individual cost records per API call
- `token_usages` - List of detailed token usage records per API call
- `response_latencies` - List of response latency metrics per API call

For more details on the available metrics and methods, refer to the [source code](https://github.com/All-Hands-AI/agent-sdk/blob/main/openhands-sdk/openhands/sdk/llm/utils/metrics.py).

## Next Steps

- **[Conversation Costs](/sdk/guides/conversation-costs)** - Calculate costs per conversation
- **[LLM Routing](/sdk/guides/llm-routing)** - Optimize costs with smart routing
