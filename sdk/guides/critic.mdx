---
title: Critic-Based Evaluation
description: Configure agents with critics to evaluate action quality in real-time.
---

<Warning>
This feature is **EXPERIMENTAL** and may change in future releases.
</Warning>

<Note>
This example is available on GitHub: [examples/01_standalone_sdk/34_critic_example.py](https://github.com/OpenHands/software-agent-sdk/blob/main/examples/01_standalone_sdk/34_critic_example.py)
</Note>

Use a critic to evaluate agent actions in real-time. The critic scores are displayed in the conversation visualizer, helping you understand the quality of agent decisions.

```python icon="python" expandable examples/01_standalone_sdk/34_critic_example.py
"""Example demonstrating critic-based evaluation of agent actions.

This is EXPERIMENTAL.

This shows how to configure an agent with a critic to evaluate action quality
in real-time. The critic scores are displayed in the conversation visualizer.

For All-Hands LLM proxy (llm-proxy.*.all-hands.dev), the critic is auto-configured
using the same base_url with /vllm suffix and "critic" as the model name.
"""

import os
import re

from openhands.sdk import LLM, Agent, Conversation, Tool
from openhands.sdk.critic import APIBasedCritic
from openhands.sdk.critic.base import CriticBase
from openhands.tools.file_editor import FileEditorTool
from openhands.tools.task_tracker import TaskTrackerTool
from openhands.tools.terminal import TerminalTool


def get_required_env(name: str) -> str:
    value = os.getenv(name)
    if value:
        return value
    raise ValueError(
        f"Missing required environment variable: {name}. "
        f"Set {name} before running this example."
    )


def get_default_critic(llm: LLM) -> CriticBase | None:
    """Auto-configure critic for All-Hands LLM proxy.

    When the LLM base_url matches `llm-proxy.*.all-hands.dev`, returns an
    APIBasedCritic configured with:
    - server_url: {base_url}/vllm
    - api_key: same as LLM
    - model_name: "critic"

    Returns None if base_url doesn't match or api_key is not set.
    """
    base_url = llm.base_url
    api_key = llm.api_key
    if base_url is None or api_key is None:
        return None

    # Match: llm-proxy.{env}.all-hands.dev (e.g., staging, prod, eval)
    pattern = r"^https?://llm-proxy\.[^./]+\.all-hands\.dev"
    if not re.match(pattern, base_url):
        return None

    return APIBasedCritic(
        server_url=f"{base_url.rstrip('/')}/vllm",
        api_key=api_key,
        model_name="critic",
    )


llm_api_key = get_required_env("LLM_API_KEY")

llm = LLM(
    model=os.getenv("LLM_MODEL", "anthropic/claude-sonnet-4-5-20250929"),
    api_key=llm_api_key,
    base_url=os.getenv("LLM_BASE_URL", None),
)

# Try auto-configuration for All-Hands proxy, fall back to explicit env vars
critic = get_default_critic(llm)
if critic is None:
    critic = APIBasedCritic(
        server_url=get_required_env("CRITIC_SERVER_URL"),
        api_key=get_required_env("CRITIC_API_KEY"),
        model_name=get_required_env("CRITIC_MODEL_NAME"),
    )


# Configure agent with critic
agent = Agent(
    llm=llm,
    tools=[
        Tool(name=TerminalTool.name),
        Tool(name=FileEditorTool.name),
        Tool(name=TaskTrackerTool.name),
    ],
    # Add critic to evaluate agent actions
    critic=critic,
)

cwd = os.getcwd()
conversation = Conversation(agent=agent, workspace=cwd)

conversation.send_message(
    "Create a file called GREETING.txt with a friendly greeting message."
)
conversation.run()

# Report cost
cost = llm.metrics.accumulated_cost
print(f"EXAMPLE_COST: {cost:.4f}")

print("\nAll done! Check the output above for 'Critic Score' in the visualizer.")
```

```bash Running the Example
cd agent-sdk
export LLM_API_KEY="your-api-key"
# For All-Hands LLM proxy, critic is auto-configured
export LLM_BASE_URL="https://llm-proxy.eval.all-hands.dev"
uv run python examples/01_standalone_sdk/34_critic_example.py
```

## How It Works

### 1. Create a Critic

The SDK provides `APIBasedCritic` for evaluating agent actions via an API endpoint:

```python
from openhands.sdk.critic import APIBasedCritic

critic = APIBasedCritic(
    server_url="https://your-critic-server.com/vllm",
    api_key="your-api-key",
    model_name="critic",
)
```

### 2. Attach Critic to Agent

Pass the critic when creating your agent:

```python
agent = Agent(
    llm=llm,
    tools=[...],
    critic=critic,  # Add critic for action evaluation
)
```

### 3. View Scores in Visualizer

When the agent runs, critic scores are displayed in the conversation visualizer, showing the quality assessment for each action.

## Auto-Configuration for All-Hands LLM Proxy

When using the All-Hands LLM proxy (`llm-proxy.*.all-hands.dev`), the critic can be auto-configured:

```python
def get_default_critic(llm: LLM) -> CriticBase | None:
    """Auto-configure critic for All-Hands LLM proxy."""
    base_url = llm.base_url
    api_key = llm.api_key
    if base_url is None or api_key is None:
        return None

    # Match: llm-proxy.{env}.all-hands.dev
    pattern = r"^https?://llm-proxy\.[^./]+\.all-hands\.dev"
    if not re.match(pattern, base_url):
        return None

    return APIBasedCritic(
        server_url=f"{base_url.rstrip('/')}/vllm",
        api_key=api_key,
        model_name="critic",
    )
```

## Configuration Options

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `LLM_API_KEY` | API key for the LLM | Yes |
| `LLM_MODEL` | Model to use (default: `anthropic/claude-sonnet-4-5-20250929`) | No |
| `LLM_BASE_URL` | Base URL for LLM API | No |
| `CRITIC_SERVER_URL` | Critic server URL (if not using auto-config) | Conditional |
| `CRITIC_API_KEY` | Critic API key (if not using auto-config) | Conditional |
| `CRITIC_MODEL_NAME` | Critic model name (if not using auto-config) | Conditional |

### Custom Critic Implementation

You can implement your own critic by extending `CriticBase`:

```python
from openhands.sdk.critic.base import CriticBase

class MyCustomCritic(CriticBase):
    def evaluate(self, action, context) -> float:
        # Your evaluation logic here
        return score  # Return a score between 0 and 1
```

## Next Steps

- **[Iterative Refinement](/sdk/guides/iterative-refinement)** - Use critics in iterative improvement workflows
- **[Agent Delegation](/sdk/guides/agent-delegation)** - Combine critics with multi-agent systems
- **[Observability](/sdk/guides/observability)** - Monitor agent performance with external tools
