---
title: "Data & Analysis Prompts"
sidebarTitle: "Data & Analysis"
description: "Ready-to-use prompts for data processing and analysis tasks"
---

# Data & Analysis Prompts

Copy and customize these prompts for data processing and analysis tasks.

## Data Processing Prompts

### Create Data Pipeline

```markdown
Create a data pipeline to process [data_source].

Input: [file format, API endpoint, database table]
Output: [destination and format]

Processing steps:
1. Extract from [source]
2. Validate [validation rules]
3. Transform [transformations needed]
4. Load to [destination]

Error handling:
- Log invalid records
- Continue on non-critical errors
- Fail fast on critical issues

Add tests with sample data.
```

### Data Validation

```markdown
Add data validation for [model/schema].

Validate:
- Required fields: [list]
- Format validations: [email, phone, etc]
- Range validations: [min/max values]
- Custom rules: [business logic]

On validation failure:
- Return specific error messages
- Include field name and invalid value
- Support multiple errors per record
```

### CSV/JSON Processing

```markdown
Create a utility to process [file_type] files.

Features:
- Read large files efficiently (streaming)
- Handle encoding issues
- Skip header row (configurable)
- Map columns to model

Input: [file path or file object]
Output: [list of objects, generator, etc]

Include:
- Progress callback for large files
- Error handling for malformed rows
- Type conversion (strings to dates, numbers)
```

## Analysis Prompts

### Generate Report

```markdown
Create a report generator for [report_name].

Data sources:
- [source 1]
- [source 2]

Metrics to calculate:
- [metric 1]: [formula/logic]
- [metric 2]: [formula/logic]
- [metric 3]: [formula/logic]

Output format: [PDF/HTML/JSON]

Include:
- Summary statistics
- Trends over time
- Comparison with previous period
```

### Data Aggregation

```markdown
Create aggregation functions for [data_type].

Aggregations needed:
1. Group by [field] and calculate:
   - Count
   - Sum of [amount_field]
   - Average of [value_field]
   
2. Time-based aggregation:
   - Daily/Weekly/Monthly totals
   - Running averages
   - Period-over-period comparison

Output as [DataFrame/dict/JSON].
```

### Statistical Analysis

```markdown
Add statistical analysis to [data_module].

Calculate:
- Descriptive stats (mean, median, std, min, max)
- Distribution analysis
- Correlation between [field1] and [field2]
- Outlier detection

Visualization:
- Histogram for distribution
- Scatter plot for correlation
- Box plot for outliers

Use [pandas/numpy/scipy] following project conventions.
```

## Database Prompts

### Write SQL Query

```markdown
Write a SQL query to [objective].

Tables involved:
- [table1]: [columns]
- [table2]: [columns]

Requirements:
- [filter conditions]
- [join logic]
- [grouping/aggregation]
- [sorting]

Performance considerations:
- Use indexes on [fields]
- Limit results to [N]
- Avoid full table scans
```

### Database Migration

```markdown
Create a database migration for [change].

Changes:
- Add column: [column_name] [type] to [table]
- Modify column: [changes]
- Add index on: [columns]
- Create table: [schema]

Include:
- Up migration
- Down migration (rollback)
- Data migration script if needed

Test with sample data before and after.
```

### Query Optimization

```markdown
Optimize this slow query:

```sql
[paste query]
```

Current performance: [time/rows scanned]
Target: [desired performance]

Analyze:
- Missing indexes
- Inefficient joins
- Subquery vs join opportunities
- Query plan analysis

Provide optimized query and explanation.
```

## Data Integration Prompts

### API Data Sync

```markdown
Create a sync job for [external_api] data.

API details:
- Endpoint: [url]
- Auth: [method]
- Rate limit: [requests/minute]

Sync logic:
- Full sync: [when/how]
- Incremental sync: [based on timestamp/id]
- Conflict resolution: [strategy]

Error handling:
- Retry on transient errors
- Log permanent failures
- Alert on threshold exceeded
```

### ETL Script

```markdown
Create an ETL script for [data_source] to [destination].

Extract:
- Source: [database/API/file]
- Query/Filter: [conditions]
- Frequency: [schedule]

Transform:
- [transformation 1]
- [transformation 2]
- Data cleaning rules

Load:
- Destination: [target]
- Mode: [insert/upsert/replace]
- Batch size: [number]

Add logging for monitoring and debugging.
```

## Quick Reference

| Task | Key Elements |
|------|--------------|
| Pipeline | Source, destination, steps, error handling |
| Validation | Rules, error messages, failure handling |
| Report | Data sources, metrics, output format |
| SQL | Tables, joins, filters, performance |
| Migration | Changes, up/down scripts, data migration |
| ETL | Extract source, transforms, load destination |

<Card title="Best Practices" icon="lightbulb" href="/use-cases/best-practices">
  Learn how to structure data tasks effectively
</Card>
