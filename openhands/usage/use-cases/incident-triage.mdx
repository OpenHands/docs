---
title: TODO-Incident Triage
description: Using OpenHands to investigate and resolve production incidents
---

When production incidents occur, speed matters. OpenHands can help you quickly investigate issues, analyze logs and errors, identify root causes, and generate fixesâ€”reducing your mean time to resolution (MTTR).

## Overview

OpenHands accelerates incident response by:

- **Rapid log analysis**: Parse and understand large volumes of logs quickly
- **Error investigation**: Trace errors through stack traces and code
- **Root cause identification**: Connect symptoms to underlying issues
- **Fix generation**: Create and test fixes for identified problems

<Note>
For production incidents, always follow your organization's incident response procedures. OpenHands is a tool to assist your investigation, not a replacement for proper incident management.
</Note>

## Incident Investigation

### Log Analysis

OpenHands can analyze logs to identify patterns and anomalies:

```
Analyze these application logs for the incident that occurred at 14:32 UTC:

1. Identify the first error or warning that appeared
2. Trace the sequence of events leading to the failure
3. Find any correlated errors across services
4. Identify the user or request that triggered the issue
5. Summarize the timeline of events
```

**Log analysis capabilities:**

| Log Type | Analysis Capabilities |
|----------|----------------------|
| Application logs | Error patterns, exception traces, timing anomalies |
| Access logs | Traffic patterns, slow requests, error responses |
| System logs | Resource exhaustion, process crashes, system errors |
| Database logs | Slow queries, deadlocks, connection issues |

**Example prompt for multi-service logs:**

```
I have logs from three services during an incident:
- api-gateway.log (attached)
- user-service.log (attached)
- database-slow.log (attached)

The incident: Users reported 504 timeout errors starting at 10:15 AM.

Correlate events across these logs and identify:
1. The originating cause
2. How it propagated through services
3. Why it resulted in 504 errors
```

### Error Tracking

Investigate errors from your error tracking system:

```
We have a Sentry alert for increased NullPointerException in the OrderService.

Here's the stack trace and context:
[paste stack trace]

Analyze this error:
1. What code path leads to this exception?
2. What input conditions could cause it?
3. When did this start occurring (check recent commits)?
4. How widespread is the impact?
```

### Root Cause Analysis

Identify the underlying cause of an incident:

```
Perform root cause analysis for this incident:

Symptoms:
- API response times increased 5x at 14:00
- Error rate jumped from 0.1% to 15%
- Database CPU spiked to 100%

Available data:
- Application metrics (Grafana dashboard attached)
- Recent deployments: v2.3.1 deployed at 13:45
- Database slow query log (attached)

Identify the root cause using the 5 Whys technique.
```

## Automated Diagnosis

### Pattern Recognition

OpenHands can recognize common incident patterns:

```
Analyze this error pattern and identify if it matches known issues:

Error: Connection refused to redis:6379
Frequency: 500 occurrences in last 10 minutes
Affected services: user-cache, session-store

Check for:
1. Network connectivity issues
2. Redis server health
3. Connection pool exhaustion
4. Recent configuration changes
```

**Common patterns OpenHands identifies:**

- **Connection pool exhaustion**: Increasing connection errors followed by complete failure
- **Memory leaks**: Gradual memory increase leading to OOM
- **Cascading failures**: One service failure triggering others
- **Thundering herd**: Simultaneous requests overwhelming a service
- **Split brain**: Inconsistent state across distributed components

### Stack Trace Analysis

Deep dive into stack traces:

```
Analyze this stack trace from our production error:

[paste full stack trace]

1. Identify the exception type and message
2. Trace back to our code (not framework code)
3. Identify the likely cause
4. Check if this code path has changed recently
5. Suggest a fix
```

**Multi-language support:**

<Tabs>
  <Tab title="Java">
    ```
    Analyze this Java exception:
    
    java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:3210)
        at java.util.ArrayList.grow(ArrayList.java:265)
        at com.myapp.DataProcessor.loadAllRecords(DataProcessor.java:142)
    
    Identify:
    1. What operation is consuming memory?
    2. Is there a memory leak or just too much data?
    3. What's the fix?
    ```
  </Tab>
  <Tab title="Python">
    ```
    Analyze this Python traceback:
    
    Traceback (most recent call last):
      File "app/api/orders.py", line 45, in create_order
        order = OrderService.create(data)
      File "app/services/order.py", line 89, in create
        inventory.reserve(item_id, quantity)
    AttributeError: 'NoneType' object has no attribute 'reserve'
    
    What's None and why?
    ```
  </Tab>
  <Tab title="JavaScript">
    ```
    Analyze this Node.js error:
    
    TypeError: Cannot read property 'map' of undefined
        at processItems (/app/src/handlers/items.js:23:15)
        at async handleRequest (/app/src/api/router.js:45:12)
    
    What's undefined and how should we handle it?
    ```
  </Tab>
</Tabs>

### System State Investigation

Investigate the system state during an incident:

```
Help me investigate system state during the incident:

Current observations:
- CPU: 95% (normally 30%)
- Memory: 85% (normally 60%)
- Disk I/O: 100% utilization
- Network: Normal

Running processes (top output attached)
Open files (lsof output attached)
Network connections (netstat output attached)

What's causing the high resource usage?
```

## Rapid Response

### Quick Fix Generation

Generate fixes for identified issues:

```
We've identified the root cause: a missing null check in OrderProcessor.java line 156.

Generate a fix that:
1. Adds proper null checking
2. Logs when null is encountered
3. Returns an appropriate error response
4. Includes a unit test for the edge case
5. Is minimally invasive for a hotfix
```

**Example fix workflow:**

```
# Step 1: Identify the issue
The NullPointerException occurs because `user.getPreferences()` 
can return null for new users.

# Step 2: Generate the fix
```java
// Before
String theme = user.getPreferences().getTheme();

// After
String theme = Optional.ofNullable(user.getPreferences())
    .map(UserPreferences::getTheme)
    .orElse(DEFAULT_THEME);
```

# Step 3: Generate the test
```java
@Test
void shouldUseDefaultThemeWhenPreferencesNull() {
    User user = new User();
    user.setPreferences(null);
    
    String theme = userService.getTheme(user);
    
    assertEquals(DEFAULT_THEME, theme);
}
```
```

### Hotfix Deployment

Generate deployment commands for hotfixes:

```
Generate hotfix deployment steps for the OrderProcessor fix:

Environment: Kubernetes on AWS EKS
Current version: v2.3.1
Hotfix version: v2.3.2

Include:
1. Build commands
2. Image push commands
3. Rollout commands
4. Verification steps
5. Rollback commands if needed
```

### Documentation Generation

Document the incident for post-mortem:

```
Generate an incident report for the OrderProcessor outage:

Timeline:
- 14:00: First errors reported
- 14:15: Incident declared
- 14:30: Root cause identified
- 14:45: Hotfix deployed
- 15:00: Service restored

Include:
1. Executive summary
2. Timeline of events
3. Root cause analysis
4. Impact assessment
5. Remediation steps taken
6. Follow-up action items
```

## Best Practices

### Effective Investigation Prompts

Provide context for faster diagnosis:

```
Investigate this production incident:

Service: payment-gateway
Environment: production-us-east-1
Incident start: 2024-01-15 14:32 UTC
Severity: P1 (customer-impacting)

Symptoms:
- 30% of payment requests failing
- Error: "Transaction timeout after 30s"
- Affected: All payment methods

Recent changes:
- v3.2.1 deployed at 14:00 (30 min before incident)
- Database maintenance window ended at 14:30

Available data:
- Application logs (CloudWatch)
- APM traces (Datadog)
- Database metrics (RDS dashboard)
```

### Investigation Checklist

Use this checklist when investigating:

1. **Scope the impact**
   - How many users affected?
   - What functionality is broken?
   - What's the business impact?

2. **Establish timeline**
   - When did it start?
   - What changed around that time?
   - Is it getting worse or stable?

3. **Gather data**
   - Application logs
   - Infrastructure metrics
   - Recent deployments
   - Configuration changes

4. **Form hypotheses**
   - List possible causes
   - Rank by likelihood
   - Test systematically

5. **Implement fix**
   - Choose safest fix
   - Test before deploying
   - Monitor after deployment

### Common Pitfalls

<Warning>
Avoid these common incident response mistakes:

- **Jumping to conclusions**: Gather data before assuming the cause
- **Changing multiple things**: Make one change at a time to isolate effects
- **Not documenting**: Record all actions for the post-mortem
- **Ignoring rollback**: Always have a rollback plan before deploying fixes
</Warning>

## Examples

### Database Performance Incident

```
Investigate a database performance incident:

Symptoms:
- API latency increased from 100ms to 5s
- Database CPU at 100%
- Connection pool exhausted

Data available:
- Slow query log (attached)
- APM traces showing database calls
- Recent schema migration completed 2 hours ago

Find the problematic query and suggest a fix.
```

### Memory Leak Investigation

```
Investigate a memory leak in our Node.js application:

Observations:
- Memory usage grows 100MB/hour
- Restarts temporarily fix the issue
- Started after v2.1.0 deployment

Available:
- Heap snapshot from before restart
- Git diff of v2.0.9 to v2.1.0
- Memory usage graphs

Identify the leak and suggest a fix.
```

### Cascading Failure Analysis

```
Analyze this cascading failure:

Timeline:
- 10:00: Redis primary failed over to replica
- 10:01: Cache miss rate increased 10x
- 10:02: Database connections exhausted
- 10:03: API gateway started returning 503
- 10:05: All services unavailable

Help me understand:
1. Why did a Redis failover cause total outage?
2. Where are the missing circuit breakers?
3. How do we prevent this in the future?
```

## Related Resources

- [Session Insights](/openhands/usage/product-guides/session-insights) - Analyze session performance
- [Troubleshooting](/openhands/usage/troubleshooting/troubleshooting) - Common OpenHands issues
- [Prompting Best Practices](/openhands/usage/tips/prompting-best-practices) - Write effective prompts
